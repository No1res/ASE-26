### 节点中心性，入度出度->权重?
1. 中心性高，上下文优先级高，提供更详细的snippets
2. 中心性低，上下文优先级低，压缩

### **中心性 ≠ 一定要多给上下文，而是要换“形态”给**
现在的想法是：
> 中心性高 → 优先级高 → 给更详细的 snippets

这里面有个隐藏假设：**越核心的模块，越应该给“更多代码”**。
但很多仓库里，中心节点往往是：
- 各种 **util / helpers / wrappers / logging / error handling**
- 或者一个 **超长的“大怪物模块”**

这些东西如果原样塞进 prompt，反而会**稀释**目标任务的信号，让模型被大量通用逻辑分散注意力。更合理的策略可能是：
- 对 _高中心性模块_：
    - 少给代码，多给 **接口级 summary + 典型用法**，把它当“黑盒库”的文档来用
- 对 _低中心性但和目标强相关的局部模块_：
    - 多给代码，因为这些往往是“特殊业务逻辑”

> **Insight：中心性高 ≠ 给更多代码，而是更应该给「更抽象、更浓缩」的描述。**
> **中心性可以决定“snippet 形态”（code vs summary），而不仅是权重。**

### 冗余依赖
1. 从目标函数出发，向外发散的hop数，这个hop数可以作为一个惩罚项，但是如果多hop之后到达的是中心性极高的模块呢？
2. 需要依赖AST解析，筛选出实际用到的模块中的某个具体函数
3. 利用deps图依赖边找到需要进行筛选的目标模块

### 基于hop距离的递减函数做context prune？
1. 定义一个跨文件跳数的hop_distance
	1. hop=0：目标文件本身
	2. hop=1：目标文件import的模块
	3. hop=2：目标文件import再import的模块
2. 越远的hop价值越低（真的如此吗？中心性特别强的模块怎么办）
3. 设置一个得分：```得分 = α*(1 / hop_distance) + β*函数相关度 + γ*中心性 - δ*长度惩罚```
### **hop 距离本质上是「拓扑距离」，但你真正要的是「*语义距离*」**
你现在的 hop 惩罚是很自然的一阶想法，但有两个点值得特别强调：
1. 有些 **远距离节点**（hop≥2）其实是：
    - 全局配置        
    - schema 定义
    - 抽象基类
        这类东西虽然在依赖图上很远，但**语义上是“单一真相源”**，对很多函数都很关键。
2. 相反，有些 hop=1 的模块只是：
    - 被 import 进来但没用        
    - 或者只用到一个常量，意义不大

这说明：**单靠 hop 不能代表“上下文价值”，你需要的是“从目标函数出发的个性化重要性”。**
比较更有信息论味道的做法是：
> 把依赖图当成一个图结构，在上面做 **personalized PageRank / random walk**，
> 起点是目标函数所在模块，边权来自 AST 使用频率 → 得到一个“图上的相关度分布”。

这样：
- hop 只是自然衰减的一部分
- 那些虽然远但被大量路径反复走过的“schema/config/抽象接口”，仍然可以获得高分
> **Insight：把 hop 惩罚升级为“个性化 PageRank/图传播”，用“从目标函数发散的概率质量”刻画真正的语义距离。**

### 图揭示哪些模块属于同一语义空间？
1. 子图裁剪算法
2. 从目标文件节点出发，展开至hop=1的邻接点
3. 对hop=1的文件执行压缩
4. hop>1的节点按规则过滤
### **语义子图不仅是裁剪单位，也是「角色分配单位」**
> 图揭示哪些模块属于同一语义空间 → 子图裁剪

这是一个很好的起点，但可以再往前一步：在一个语义子图里，不同节点扮演的**角色**是不同的：
- “定义规范/类型的模块”（schemas/models）
- “业务流程的 orchestrator”    
- “提供工具函数的 util”
- “配置/常量/枚举”    

对 LLM 来说，每种角色对应的**最佳呈现方式**是不一样的：
- schema/module → 类型定义 + 字段说明（summary 即可）
- orchestrator → 调用链概览 + 关键分支（半代码、半 summary）
- util → 函数签名 + 典型调用样例
- config → 值范围 +默认值

> **Insight：语义子图不仅是一坨“相关文件”，还天然划分出不同角色。**
> **你的上下文构造可以从「选文件」升级到「*为每个角色选合适的展示粒度*」。**

### LLM引入，只负责压缩/打分（Token消耗怎么办？）
现在已经有了静态函数，负责找到目标函数，通过deps图与AST找到候选snippet，每个snippet有静态的特征。
1. LLM打分：对候选的snippet，可以喂给打分LLM（可以训练一个小模型？）
	1. 让llm回答：在补全下面这个目标函数时，片段 A/B/C 对理解其行为是否重要？请分别给 0~5 的相关度评分，并简要说明理由。
	2. 拿到分数时，作为新的LLM_score，这个llm分数可以以一定权重加入最终得分计算。
	3. score_final = λ * static_score + (1 - λ) * LLM_score
2. LLM压缩：对每个入选的snippet，如果需要压缩：
	1. 采取prompt-engineering，使用LLM压缩代码为自然语言：
	2. prompt（draft）：“用 **3 行以内**，总结这段代码提供的功能、主要参数和返回值，不要讲实现细节，保留对接口行为有帮助的内容。”
	3. 压缩后的文本可以直接组织到prompt之中
3. LLM启发式生成候选：
	1. 给prompt让模型判断：为了实现这个函数，你觉得还需要知道哪些类型/函数/配置？请用 identifier 列表返回。
	2. 拿到列表之后，可以直接去搜AST，拿到相似的定义形成候选集


### **现在的打分是线性加权，但你的问题本质更像「多目标约束的选择问题」**
```score = α*(1/hop) + β*函数相关度 + γ*中心性 - δ*长度惩罚```

默认是线性可加，但你真正关心的其实是：
- **必要性**（缺它就很容易写错）
- **冗余度**（这段信息是不是已经被别的 snippet 隐含了）
- **混淆度**（引入它会不会把模型带偏）

这些不是简单线性叠加能表达的。更自然的思路：
1. 先做**硬约束过滤**：
    - hop 太远 & 中心性也不高 → 直接不要
    - 长度过大但相关度不够高 → 丢弃
2. 在剩下的里做**局部贪心**：
    - 每次选“能带来最大边际收益 / 新信息”的 snippet

可类比：
- 经典信息检索里的 **Maximal Marginal Relevance (MMR)**：
    - 同时考虑 “和目标的相关性” & “与已经选中集合的差异度”
  
> **Insight：把 snippet 选择当成「信息覆盖」问题，而不是单点打分排序问题。**
> **你要惩罚“互相太相似”的 snippets，而不仅是惩罚单个 snippet 的长度。**

### **关于 Token 成本：把 LLM 用在「少量关键决策」，而不是所有 snippet 上**
> “LLM 打分/压缩本身也要 token”

这意味着不能对所有 snippet 都无脑调用。结合你的静态规则，可以做一个「两阶段决策」：
1. **静态规则先把候选分三类：Must / Uncertain / Probably-Noise**
    - Must：同文件+直接调用+共享数据的部分，直接入选，不用 LLM
    - Probably-Noise：hop 远、中心性低、相关度低，直接丢弃
    - Uncertain：特征模糊（例如 hop=1 但调用不多，名字有点像）的那些
2. **只对 Uncertain 那一小撮，用 LLM 进行打分/压缩**

这样：
- 绝大部分 snippet 的选择由静态规则完成，几乎零成本    
- LLM 仅在“人也拿不准”的边界样本上做判决，把算力用在刀刃上

> **Insight：LLM 应该被当成“高成本的专家裁决器”，**
> **放在静态规则无法 confidently 决策的灰色地带，而不是全局打分机器。**


这个观点背后其实是一套**高层语义建模策略**，远超“给 LLM 更多上下文”，而是“让上下文成为结构化、分角色、信息密度最优的知识图”。

---

### **🎯 核心 Insight：**
#### **“最小充分上下文”不取决于代码量，而取决于角色适配的上下文形态**
换句话说：
> **真正重要的不是“给代码库的哪些部分”，而是“以什么形态、什么粒度给它们”。**
> 这才是决定 LLM 能否理解任务的关键。

#### **🔍 为什么“角色粒度”比“文件粒度”更关键？**
如果你只做“模块裁剪”（判断哪些文件要/不要放进 prompt），本质上你是在做信息“选取”，但 LLM 的理解需要的不仅是选取，还包括**信息的加工方式**。

例如：
- schemas 往往行数不多，但关系复杂
- orchestrators 重要逻辑高度流程化
- utils 行数少、功能明确、示例比实现更重要
- config 不需要源码，需要的是“值域约束和语义”

所以如果你把它们都以“长代码片段”形式塞进去，模型反而难理解。这就是为什么你需要“角色适配展示粒度”——不同代码角色需要不同“信息压缩策略”。

#### **🧩 Insight 1：代码角色是「信息压缩策略」的自然分界点**
不同角色的“信息价值密度”不一样：

| **角色**             | **信息密度** | **对模型的关键信息**        | **最佳呈现方式**               |
| ------------------ | -------- | ------------------- | ------------------------ |
| **schema / model** | 超高       | 字段含义、类型约束、字段间关系     | 文本 summary / 字段表格（无需源码）  |
| **orchestrator**   | 中等偏高     | 关键分支、调用链、异常路径       | 控制流 summary + 局部源码       |
| **utility**        | 中等       | 函数参数 + 副作用 + 典型调用案例 | 函数签名 + usage 示例（不需要内部逻辑） |
| **config / enum**  | 超低（通常很短） | 值域/默认值              | 直接列出来即可（无需源码）            |

这意味着：

> **即使两个模块等价“重要”，它们所需的内容长度和形态完全不同。**
> **‘重要性’ ≠ ‘需要放大量代码’。**
> **‘角色’ 决定展示方式远比‘模块级重要性’更关键。**

这是构造最小上下文时一个被普遍忽略的重要事实。

#### **🧩 Insight 2：上下文应该是“一段代码 + 多段摘要”的混合体，而不是统一颗粒度**

LLM 阅读效率最差的是“长代码片段”，最擅长处理的是 **结构化 + 文本型描述**。
所以：
- 对 orchestrator：给**骨架代码** + **控制流 summary**
- 对 schema：直接给“字段描述表”就够了
- 对 utils：只要“签名 + usage”比“源码本体”价值高很多

换句话说：
> **代码库的不同角色应该对应不同的 token 形式，**

> **不同的信息形态应该共享同一个上下文预算。**

这个 insight 能极大提升“每个 token 的效能”。

---

#### **🧩 Insight 3：模型不是在“看代码”，而是在“构建 mental model（心智模型）”**

LLM 在补全函数时真正想做的事不是：
- 逐行模拟执行
- 精确分析依赖图
- 理解每个函数的字节级实现

而是：
> **构建一个足够贴近真实库行为的“心智模型”，用于推断目标函数应如何写。**

这个心智模型依赖 **结构、约定、语义模式、数据契约**，而不是代码本身。

所以：
- schema 的价值是 “告诉模型对象长什么样”
- orchestrator 的价值是 “告诉模型流程的骨架”
- utils 的价值是 “告诉模型有什么工具可以复用”
- config 的价值是 “告诉模型预期的参数范围与配置约定”

这些并不是“代码片段”，而是 **结构知识**。

> **Insight：上下文构造的目标不是“展示代码”，而是“为模型构建正确的 mental model”。**

> **角色粒度 = 结构知识的分区，而不是文件的分区。**

---

#### **🧩 Insight 4： “角色 → 展现粒度”可以复用到任意语言、任意仓库，是跨语言的本质抽象**
例如：
- Java 的 POJO / DTO 就对应 schema
- Rust 的 struct / trait 同样是 schema
- C++ 的 util templates 对应 utils
- TypeScript 的 orchestrator 都是 promise chains

这个策略是语言无关的。

> **所以“依据角色选择上下文形态”是比“文件裁剪”更本质的跨语言抽象。**  

这就是为什么它能成为一个强 insight：
你不仅在优化 Python 的最小上下文，而是在构建一种可以通用的 **仓库语义建模方法**。

---

#### **🧩 Insight 5：LLM 的 token 消耗瓶颈 → 角色驱动的“信息压缩优先级”是关键突破点**

你之前提到：
- snippet 压缩
- LLM 打分
- hop-distance prune

这些都在做 **局部最优压缩**。但如果你引入“角色”：

- orchestrator 才需要混合型上下文（部分代码 + 分析）
- schema 天生适合文本 summary（最省 token）
- utils 不需要源码，只需要 usage（非常短）
- config 最便宜（几行即可）

结果是什么？

> **最终上下文不是平均裁剪，而是极度非对称裁剪：**
> **绝大部分上下文 token 来自同一两个角色，而其他角色几乎不占空间。**

这是大模型时代 _真正的 token 预算优化策略_。

---

#### **🧩 Insight 6：角色识别本身也可以由 LLM 辅助：从 AST + 文本自动推断“代码角色标签”**
而且不需要训练，只需 prompting：
> “这段代码更像：A) 类型定义、B) 工具函数、C) 业务 orchestrator、D) 配置？
> 请输出标签。”

这给你提供：
- 自动化的代码角色标注
- 自动切换为不同的上下文呈现策略
- 不依赖人工规则，不依赖人工维护仓库知识

这个 insight 是关键，因为：
> **自动角色识别 → 角色适配的上下文粒度 → 最小充分上下文的稳健性上升到“语义级别”。**
---
#### **🧩 Insight 7：未来方向：将“角色子图 + LLM summary”当作仓库级知识图谱**
你最终构建的不是 prompt，而是一个：
> **角色标注的仓库级语义图（Semantic Repository Graph）**

包含：
- 节点：角色标注的抽象实体
- 边：依赖 / 语义联结
- 节点附带：
    - 压缩摘要
    - 常用调用模式
    - 参数约定
    - 行为描述
- 小片段源码仅在必要时提供

这类图可以：
- 作为 RAG 的知识库
- 作为上下文选取器的输入
- 给任何模型复用，不限 CoderEval、InCoderEval…

> **Insight：你正在构建的不是一个“prompt 工具”，**
> **而是一种“代码库的语义压缩结构”，未来可直接变成 RAG 的知识图谱格式。**

---

#### **🧩 总结一句话**
> **“为每个角色选合适展示粒度”不是简单的 prompt 优化，**
> **而是把代码视为“语义图中的节点”，不同节点需要不同的信息形态。**
> **这将上下文从‘文件级剪裁’提升为‘结构知识重建’，**
> **是最小上下文构造中最具洞察力的方向。**